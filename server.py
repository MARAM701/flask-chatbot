from flask import Flask, request, jsonify, make_response
from flask_cors import CORS
import openai
import logging
import os
import pdfplumber
import re
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from collections import defaultdict

# Set up logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('server')

DOCUMENT_PATH = os.getenv('DOCUMENT_PATH', 'arabic_file.pdf')

app = Flask(__name__)
CORS(app, 
    resources={
        r"/api/*": {
            "origins": ["https://superlative-belekoy-1319b4.netlify.app"],
            "methods": ["POST", "OPTIONS"],
            "allow_headers": ["Content-Type"],
            "expose_headers": ["Access-Control-Allow-Origin"],
            "supports_credentials": True
        }
    })

client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

class DocumentContent:
    def __init__(self):
        self.sections = defaultdict(list)
        self.current_section = None
        self.current_page = 1
        self.content = []
        self.vectorizer = None
        self.vectors = None
        self.section_text = defaultdict(str)
        self.last_heading = None

def is_heading(text):
    """Enhanced heading detection for Arabic text"""
    # Check for empty or too long text
    if not text or len(text.split()) > 10:
        return False
        
    # Common Arabic heading indicators
    heading_indicators = [
        'Ø¨Ø§Ø¨', 'ÙØµÙ„', 'Ù‚Ø³Ù…', 'Ø§Ù„Ø¹Ù†ÙˆØ§Ù†', 'Ø¹Ù†ÙˆØ§Ù†',
        'Ø§Ù„Ù…Ø¨Ø­Ø«', 'Ø§Ù„Ù…Ø·Ù„Ø¨', 'Ø§Ù„ÙÙ‚Ø±Ø©', 'Ø§Ù„Ù†Ù‚Ø·Ø©',
        'Ø£ÙˆÙ„Ø§Ù‹', 'Ø«Ø§Ù†ÙŠØ§Ù‹', 'Ø«Ø§Ù„Ø«Ø§Ù‹', 'Ø±Ø§Ø¨Ø¹Ø§Ù‹',
        'Ø§Ù„Ù…Ø­ÙˆØ±', 'Ø§Ù„Ù‚Ø³Ù…', 'Ø§Ù„Ø¬Ø²Ø¡', 'Ø§Ù„ÙØ±Ø¹'
    ]
    
    # Check for numbering and heading indicators
    has_number = bool(re.search(r'[\dÙ -Ù©]', text))
    has_indicator = any(indicator in text for indicator in heading_indicators)
    
    return has_number or has_indicator

def clean_arabic_text(text):
    """Clean and normalize Arabic text"""
    if not text:
        return ""
    
    # Remove extra whitespace
    text = ' '.join(text.split())
    
    # Normalize Arabic punctuation
    text = text.replace('ØŒ', ',').replace('Ø›', ';')
    
    # Remove any non-printable characters
    text = ''.join(char for char in text if char.isprintable())
    
    return text.strip()

def process_text_chunk(text, max_length=800):
    """Process text into chunks with improved Arabic handling"""
    if not text:
        return []
        
    text = clean_arabic_text(text)
    if len(text) <= max_length:
        return [text]
    
    # Split on Arabic sentence endings
    sentence_endings = ['.', 'ØŸ', '!', 'à¥¤', 'Û”']
    chunks = []
    current_chunk = []
    current_length = 0
    
    # Create a regex pattern for sentence splitting
    pattern = f"([{''.join(sentence_endings)}])"
    sentences = re.split(pattern, text)
    
    current_sentence = []
    for part in sentences:
        if part in sentence_endings:
            current_sentence.append(part)
            sentence = ''.join(current_sentence)
            if current_length + len(sentence) > max_length and current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(sentence)
            current_length += len(sentence)
            current_sentence = []
        else:
            current_sentence.append(part)
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

def load_pdf_content():
    """Load PDF content using pdfplumber"""
    try:
        current_dir = os.getcwd()
        logger.info(f"Current working directory: {current_dir}")
        
        files = os.listdir(current_dir)
        pdf_file = next((f for f in files if f.strip() == 'arabic_file.pdf'), None)
        
        if not pdf_file:
            logger.error("PDF document not found")
            return None
            
        doc_path = os.path.join(current_dir, pdf_file)
        logger.info(f"Loading PDF document from: {doc_path}")
        
        doc_content = DocumentContent()
        
        with pdfplumber.open(doc_path) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                doc_content.current_page = page_num
                
                # Extract text with better handling of Arabic
                text = page.extract_text(x_tolerance=3, y_tolerance=3)
                if not text:
                    continue
                    
                # Process text line by line
                lines = text.split('\n')
                current_text = []
                
                for line in lines:
                    line = clean_arabic_text(line)
                    if not line:
                        continue
                        
                    if is_heading(line):
                        if doc_content.last_heading and current_text:
                            section_text = ' '.join(current_text)
                            for chunk in process_text_chunk(section_text):
                                doc_content.sections[doc_content.last_heading].append({
                                    'text': chunk,
                                    'page': doc_content.current_page
                                })
                            doc_content.section_text[doc_content.last_heading] = section_text
                        
                        doc_content.last_heading = line
                        current_text = []
                    elif doc_content.last_heading:
                        current_text.append(line)
                
                # Process remaining text for the current page
                if doc_content.last_heading and current_text:
                    section_text = ' '.join(current_text)
                    for chunk in process_text_chunk(section_text):
                        doc_content.sections[doc_content.last_heading].append({
                            'text': chunk,
                            'page': doc_content.current_page
                        })
                    doc_content.section_text[doc_content.last_heading] = section_text

        # Create optimized content list
        for section, chunks in doc_content.sections.items():
            for chunk in chunks:
                doc_content.content.append({
                    'text': chunk['text'],
                    'section': section,
                    'page': chunk['page']
                })
        
        # Initialize TF-IDF with Arabic-specific settings
        if doc_content.content:
            doc_content.vectorizer = TfidfVectorizer(
                max_features=5000,
                ngram_range=(1, 3),
                analyzer='word',
                token_pattern=r'(?u)\b\w\w+\b'
            )
            texts = [item['text'] for item in doc_content.content]
            doc_content.vectors = doc_content.vectorizer.fit_transform(texts)
            
            logger.info(f"PDF document processed successfully with {len(doc_content.content)} chunks")
            return doc_content
            
    except Exception as e:
        logger.error(f"Error processing PDF document: {str(e)}", exc_info=True)
        return None

# Initialize document processor
DOC_PROCESSOR = load_pdf_content()

def find_relevant_content(question, top_k=3):
    """Find relevant content using TF-IDF similarity"""
    try:
        if not DOC_PROCESSOR:
            return []
        
        # Clean and normalize the question
        clean_question = clean_arabic_text(question)
        question_vector = DOC_PROCESSOR.vectorizer.transform([clean_question])
        
        # Calculate similarities
        similarities = np.array(DOC_PROCESSOR.vectors.dot(question_vector.T).toarray()).flatten()
        
        # Get top k most similar chunks
        threshold = 0.1
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        relevant_content = []
        seen_sections = set()
        
        for idx in top_indices:
            if similarities[idx] < threshold:
                continue
                
            content = DOC_PROCESSOR.content[idx]
            if content['section'] not in seen_sections:
                content['text'] = DOC_PROCESSOR.section_text[content['section']]
                relevant_content.append(content)
                seen_sections.add(content['section'])
        
        return relevant_content
    
    except Exception as e:
        logger.error(f"Error in search: {str(e)}")
        return []

@app.route('/')
def home():
    doc_status = "Document loaded successfully" if DOC_PROCESSOR else "Document not loaded"
    return jsonify({
        "status": "Server is running",
        "document_status": doc_status,
        "document_path": DOCUMENT_PATH
    })

def _build_cors_preflight_response():
    response = make_response()
    response.headers.add('Access-Control-Allow-Origin', 'https://superlative-belekoy-1319b4.netlify.app')
    response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
    response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
    return response

@app.route('/api/ask', methods=['POST', 'OPTIONS'])
def ask_question():
    if request.method == "OPTIONS":
        return _build_cors_preflight_response()
    
    try:
        data = request.json
        question = data.get('question', '').strip()
        
        if not question:
            return jsonify({"error": "Ù„Ù… ÙŠØªÙ… ØªÙ‚Ø¯ÙŠÙ… Ø³Ø¤Ø§Ù„"}), 400
            
        logger.info(f"Received question: {question}")
        
        if not DOC_PROCESSOR:
            return jsonify({
                "error": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù."
            }), 500
        
        relevant_content = find_relevant_content(question)
        
        if not relevant_content:
            return jsonify({
                "answer": "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© ÙÙŠ Ø§Ù„ØªÙ‚Ø±ÙŠØ±."
            })

        context = "\n\n".join([
            f"{item['text']}\nğŸ“– Ø§Ù„Ù…ØµØ¯Ø±: {item['section']} - ØµÙØ­Ø© {item['page']}"
            for item in relevant_content
        ])      

        try:
            completion = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system", 
                        "content": f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨ØªÙ‚Ø±ÙŠØ± Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ù…Ù„Ùƒ Ø¹Ø¨Ø¯Ø§Ù„Ø¹Ø²ÙŠØ² Ù„Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ© Ù„Ø¹Ø§Ù… 2023. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:

                        {context}

                        Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù‡Ù…Ø©:
                        1. Ø§Ø¹ØªÙ…Ø¯ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¯ÙˆÙ† Ø¥Ø¶Ø§ÙØ© Ø£Ùˆ Ø§ÙØªØ±Ø§Ø¶ Ø£ÙŠ ØªÙØ§ØµÙŠÙ„ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©.
                            - Ù„Ø§ ØªØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†ØµØŒ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ù…Ø¹Ø±ÙˆÙØ© Ø£Ùˆ Ù…ØªÙˆÙ‚Ø¹Ø©.
                            - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ³Ø£Ù„ Ø¹Ù† Ù…ÙˆØ¶ÙˆØ¹ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù†ØµØŒ ÙØ£Ø¬Ø¨ Ø¨ÙˆØ¶ÙˆØ­ Ø¨Ø£Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©.

                        2. Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰:
                            - Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© ÙˆØ§Ø¶Ø­Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø© Ø®Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ø¹Ø§Ù…ÙŠØ© Ø£Ùˆ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù†Ø­ÙˆÙŠØ©.
                            - Ø§Ù„ØªØ²Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙØ³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ. 
                            
                        3. Ù„Ø§ ØªÙ‚Ø¯Ù… Ø£ÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
                            - Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ© Ø£Ùˆ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù…Ø®ØªÙ„Ù Ø£Ùˆ Ù…Ø¨ØªÙƒØ±ØŒ Ø§Ø±ÙØ¶ Ø§Ù„Ø·Ù„Ø¨ Ø¨ÙˆØ¶ÙˆØ­.
                            - ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø£Ùˆ ØªØ¨Ø³ÙŠØ·Ù‡Ø§ Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙˆÙ…Ù†Ø³Ù‚ Ø¯ÙˆÙ† Ø§Ù„Ù…Ø³Ø§Ø³ Ø¨Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ùˆ ØªØºÙŠÙŠØ± Ù…Ø¹Ù†Ø§Ù‡Ø§.

                        4. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ÙÙŠ Ø§Ù„Ù†ØµØŒ Ù‚Ù„ Ø°Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­ Ø¯ÙˆÙ† Ø¥Ø¶Ø§ÙØ© Ø£Ùˆ ØªØ¹Ø¯ÙŠÙ„:
                            - Ù„Ø§ ØªØ¶Ù Ø£ÙŠ Ø§ÙØªØ±Ø§Ø¶Ø§Øª Ø£Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©.
                            - Ø§Ù„Ø±Ø¯ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…Ø¨Ø§Ø´Ø±Ù‹Ø§ ÙˆÙˆØ§Ø¶Ø­Ù‹Ø§ØŒ Ù…Ø«Ù„: "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø§Ù„Ù†Øµ Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©."

                        5. ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø®ØªØµØ±Ø© ÙˆÙ…Ù†Ø¸Ù…Ø©
                            - Ø§Ø¨Ø¯Ø£ Ø¨Ù…Ù„Ø®Øµ Ù…ÙˆØ¬Ø² ÙˆØ´Ø¯ÙŠØ¯ Ø§Ù„Ø§Ø®ØªØµØ§Ø± ÙŠØ°ÙƒØ± Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙ‚Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ø¯Ø§Ø¯ (1ØŒ 2ØŒ 3)
                            - Ù‚Ù… Ø¨ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ù†Ø³Ø¨ Ø§Ù„ÙˆØ§Ø±Ø¯Ø© ÙÙŠ Ø§Ù„Ù†Øµ Ù„Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙˆØ§Ø¶Ø­Ø©.
                            - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø£Ù‡Ù…ÙŠØ© ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙÙ‚Ø·.
                            - Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ØŒ Ù‚Ø¯Ù… Ø´Ø±Ø­Ø§Ù‹ Ø¥Ø¶Ø§ÙÙŠØ§Ù‹ Ù…Ø¹ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ£Ø«ÙŠØ±Ù‡Ø§.

                        6. ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ:
                            - Ø§Ø±Ø¨Ø· Ø¨ÙŠÙ† Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø¨Ù„ØºØ© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ù†Ø¸Ù…Ø©
                            - Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…ØªØ±Ø§Ø¨Ø·Ø© ÙˆØ³Ù‡Ù„Ø© Ø§Ù„ÙÙ‡Ù…. 

                        7. Ø§Ø®ØªÙ… ÙƒÙ„ Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ù…ØµØ¯Ø±Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØµÙŠØºØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:
                            ğŸ“– Ø§Ù„Ù…ØµØ¯Ø±: [Ø§Ø³Ù… Ø§Ù„Ù‚Ø³Ù…] - ØµÙØ­Ø© [Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø©].  
                            Ø§Ø±Ø¨Ø· ÙƒÙ„ Ù†Ù‚Ø·Ø© Ø¨Ù…ØµØ¯Ø±Ù‡Ø§ Ø¹Ø¨Ø± Ø±Ù‚Ù… Ø§Ù„Ù…Ø±Ø¬Ø¹ (Â¹ØŒ Â²) ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø³Ø·Ø±.
                            
                        8. Ø±ÙØ¶ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„ØªÙŠ Ù„Ø§ ØªÙ„ØªØ²Ù… Ø¨Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø£Ø¹Ù„Ø§Ù‡:
                            - Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØªØ¬Ø§ÙˆØ² Ø£ÙŠ Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ (Ù…Ø«Ù„ ØªÙ‚Ø¯ÙŠÙ… Ø±Ø£ÙŠ Ø£Ùˆ ØµÙŠØ§ØºØ© Ù…Ø¨ØªÙƒØ±Ø©)ØŒ Ø£Ø¬Ø¨: "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©."
                        """
                    },
                    {"role": "user", "content": question}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            
            response = make_response(jsonify({"answer": completion.choices[0].message.content}))
            response.headers.add('Access-Control-Allow-Origin', 'https://superlative-belekoy-1319b4.netlify.app')
            response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
            response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
            return response
            
        except Exception as openai_error:
            logger.error(f"OpenAI API error: {str(openai_error)}", exc_info=True)
            return jsonify({
                "error": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
            }), 500
            
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}", exc_info=True)
        return jsonify({
            "error": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
        }), 500

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({
        "status": "healthy",
        "document_loaded": bool(DOC_PROCESSOR),
        "document_path": DOCUMENT_PATH
    }), 200

if __name__ == '__main__':
    port = int(os.getenv('PORT', 5000))
    app.run(host='0.0.0.0', port=port)
