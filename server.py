from flask import Flask, request, jsonify, make_response
from flask_cors import CORS
import openai
import logging
import os
import PyPDF2
import re
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from collections import defaultdict

# Set up logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('server')

DOCUMENT_PATH = os.getenv('DOCUMENT_PATH', 'arabic_file.pdf')

app = Flask(__name__)
CORS(app, 
    resources={
        r"/api/*": {
            "origins": ["https://superlative-belekoy-1319b4.netlify.app"],
            "methods": ["POST", "OPTIONS"],
            "allow_headers": ["Content-Type"],
            "expose_headers": ["Access-Control-Allow-Origin"],
            "supports_credentials": True
        }
    })

client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

class DocumentContent:
    def __init__(self):
        self.sections = defaultdict(list)
        self.current_section = None
        self.current_page = 1
        self.content = []
        self.vectorizer = None
        self.vectors = None
        self.section_text = defaultdict(str)
        self.last_heading = None

def is_heading(text):
    """Enhanced heading detection for Arabic text"""
    # Check for empty or too long text
    if not text or len(text.split()) > 10:
        return False
        
    # Common Arabic heading indicators with variations
    heading_indicators = [
        'ÿ®ÿßÿ®', 'ŸÅÿµŸÑ', 'ŸÇÿ≥ŸÖ', 'ÿßŸÑÿπŸÜŸàÿßŸÜ', 'ÿπŸÜŸàÿßŸÜ',
        'ÿßŸÑŸÖÿ®ÿ≠ÿ´', 'ÿßŸÑŸÖÿ∑ŸÑÿ®', 'ÿßŸÑŸÅŸÇÿ±ÿ©', 'ÿßŸÑŸÜŸÇÿ∑ÿ©',
        'ÿ£ŸàŸÑÿßŸã', 'ÿ´ÿßŸÜŸäÿßŸã', 'ÿ´ÿßŸÑÿ´ÿßŸã', 'ÿ±ÿßÿ®ÿπÿßŸã',
        'ÿßŸÑŸÖÿ≠Ÿàÿ±', 'ÿßŸÑŸÇÿ≥ŸÖ', 'ÿßŸÑÿ¨ÿ≤ÿ°', 'ÿßŸÑŸÅÿ±ÿπ'
    ]
    
    # Check for numbering and heading indicators
    has_number = bool(re.search(r'[\dŸ†-Ÿ©]', text))
    has_indicator = any(indicator in text for indicator in heading_indicators)
    
    # Return true if either condition is met
    return has_number or has_indicator

def clean_arabic_text(text):
    """Clean and normalize Arabic text"""
    if not text:
        return ""
    
    # Remove extra whitespace
    text = ' '.join(text.split())
    
    # Normalize Arabic punctuation
    text = text.replace('ÿå', ',').replace('ÿõ', ';')
    
    # Remove any non-printable characters
    text = ''.join(char for char in text if char.isprintable())
    
    return text.strip()

def process_text_chunk(text, max_length=800):
    """Process text into chunks with improved Arabic handling"""
    if not text:
        return []
        
    text = clean_arabic_text(text)
    if len(text) <= max_length:
        return [text]
    
    # Split on Arabic sentence endings
    sentence_endings = ['.', 'ÿü', '!', '‡•§', '€î']
    chunks = []
    current_chunk = []
    current_length = 0
    
    # Create a regex pattern for sentence splitting
    pattern = f"([{''.join(sentence_endings)}])"
    sentences = re.split(pattern, text)
    
    current_sentence = []
    for part in sentences:
        if part in sentence_endings:
            current_sentence.append(part)
            sentence = ''.join(current_sentence)
            if current_length + len(sentence) > max_length and current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(sentence)
            current_length += len(sentence)
            current_sentence = []
        else:
            current_sentence.append(part)
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

def load_pdf_content():
    """Enhanced PDF content loading with better Arabic support"""
    try:
        current_dir = os.getcwd()
        logger.info(f"Current working directory: {current_dir}")
        
        files = os.listdir(current_dir)
        pdf_file = next((f for f in files if f.strip() == 'arabic_file.pdf'), None)
        
        if not pdf_file:
            logger.error("PDF document not found")
            return None
            
        doc_path = os.path.join(current_dir, pdf_file)
        logger.info(f"Loading PDF document from: {doc_path}")
        
        doc_content = DocumentContent()
        
        try:
            with open(doc_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                for page_num in range(len(pdf_reader.pages)):
                    doc_content.current_page = page_num + 1
                    page = pdf_reader.pages[page_num]
                    
                    try:
                        text = page.extract_text()
                        if not text:
                            continue
                            
                        text = clean_arabic_text(text)
                        lines = text.split('\n')
                        current_text = []
                        
                        for line in lines:
                            line = line.strip()
                            if not line:
                                continue
                                
                            if is_heading(line):
                                if doc_content.last_heading and current_text:
                                    section_text = ' '.join(current_text)
                                    for chunk in process_text_chunk(section_text):
                                        doc_content.sections[doc_content.last_heading].append({
                                            'text': chunk,
                                            'page': doc_content.current_page
                                        })
                                    doc_content.section_text[doc_content.last_heading] = section_text
                                
                                doc_content.last_heading = line
                                current_text = []
                            elif doc_content.last_heading:
                                current_text.append(line)
                        
                        # Process remaining text for the current page
                        if doc_content.last_heading and current_text:
                            section_text = ' '.join(current_text)
                            for chunk in process_text_chunk(section_text):
                                doc_content.sections[doc_content.last_heading].append({
                                    'text': chunk,
                                    'page': doc_content.current_page
                                })
                            doc_content.section_text[doc_content.last_heading] = section_text
                            
                    except Exception as page_error:
                        logger.error(f"Error processing page {page_num + 1}: {str(page_error)}")
                        continue
                
                # Create optimized content list
                for section, chunks in doc_content.sections.items():
                    for chunk in chunks:
                        doc_content.content.append({
                            'text': chunk['text'],
                            'section': section,
                            'page': chunk['page']
                        })
                
                # Initialize TF-IDF with Arabic-specific settings
                if doc_content.content:
                    doc_content.vectorizer = TfidfVectorizer(
                        max_features=5000,
                        ngram_range=(1, 3),  # Increased to catch longer phrases
                        analyzer='word',
                        token_pattern=r'(?u)\b\w\w+\b'  # Better for Arabic
                    )
                    texts = [item['text'] for item in doc_content.content]
                    doc_content.vectors = doc_content.vectorizer.fit_transform(texts)
                    
                    logger.info(f"PDF document processed successfully with {len(doc_content.content)} chunks")
                    return doc_content
                    
        except PyPDF2.PdfReadError as pdf_error:
            logger.error(f"PDF reading error: {str(pdf_error)}")
            return None
            
    except Exception as e:
        logger.error(f"Error processing PDF document: {str(e)}", exc_info=True)
        return None

# Initialize document processor
DOC_PROCESSOR = load_pdf_content()

def find_relevant_content(question, top_k=3):
    """Enhanced content finding with better relevance"""
    try:
        if not DOC_PROCESSOR:
            return []
        
        # Clean and normalize the question
        clean_question = clean_arabic_text(question)
        question_vector = DOC_PROCESSOR.vectorizer.transform([clean_question])
        
        # Calculate similarities
        similarities = np.array(DOC_PROCESSOR.vectors.dot(question_vector.T).toarray()).flatten()
        
        # Get top k most similar chunks with higher threshold
        threshold = 0.1  # Minimum similarity threshold
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        relevant_content = []
        seen_sections = set()
        
        for idx in top_indices:
            if similarities[idx] < threshold:
                continue
                
            content = DOC_PROCESSOR.content[idx]
            if content['section'] not in seen_sections:
                content['text'] = DOC_PROCESSOR.section_text[content['section']]
                relevant_content.append(content)
                seen_sections.add(content['section'])
        
        return relevant_content
    
    except Exception as e:
        logger.error(f"Error in search: {str(e)}")
        return []

@app.route('/')
def home():
    doc_status = "Document loaded successfully" if DOC_PROCESSOR else "Document not loaded"
    return jsonify({
        "status": "Server is running",
        "document_status": doc_status,
        "document_path": DOCUMENT_PATH
    })

def _build_cors_preflight_response():
    response = make_response()
    response.headers.add('Access-Control-Allow-Origin', 'https://superlative-belekoy-1319b4.netlify.app')
    response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
    response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
    return response

@app.route('/api/ask', methods=['POST', 'OPTIONS'])
def ask_question():
    if request.method == "OPTIONS":
        return _build_cors_preflight_response()
    
    try:
        data = request.json
        question = data.get('question', '').strip()
        
        if not question:
            return jsonify({"error": "ŸÑŸÖ Ÿäÿ™ŸÖ ÿ™ŸÇÿØŸäŸÖ ÿ≥ÿ§ÿßŸÑ"}), 400
            
        logger.info(f"Received question: {question}")
        
        if not DOC_PROCESSOR:
            return jsonify({
                "error": "ÿπÿ∞ÿ±ÿßŸãÿå ŸÑŸÖ Ÿäÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸàÿ´ŸäŸÇÿ© ÿ®ÿ¥ŸÉŸÑ ÿµÿ≠Ÿäÿ≠. ÿßŸÑÿ±ÿ¨ÿßÿ° ÿßŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ÿßŸÑŸÖŸÑŸÅ."
            }), 500
        
        relevant_content = find_relevant_content(question)
        
        if not relevant_content:
            return jsonify({
                "answer": "ÿπÿ∞ÿ±Ÿãÿßÿå ŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ∞ÿßÿ™ ÿµŸÑÿ© ŸÅŸä ÿßŸÑÿ™ŸÇÿ±Ÿäÿ±."
            })

        context = "\n\n".join([
            f"{item['text']}\nüìñ ÿßŸÑŸÖÿµÿØÿ±: {item['section']} - ÿµŸÅÿ≠ÿ© {item['page']}"
            for item in relevant_content
        ])      
        try:
            completion = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system", 
                        "content": f"""ÿ£ŸÜÿ™ ŸÖÿ≥ÿßÿπÿØ ÿ∞ŸÉŸä ŸÖÿ™ÿÆÿµÿµ ŸÅŸä ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ÿ™ŸÇÿ±Ÿäÿ± ŸÖÿØŸäŸÜÿ© ÿßŸÑŸÖŸÑŸÉ ÿπÿ®ÿØÿßŸÑÿπÿ≤Ÿäÿ≤ ŸÑŸÑÿπŸÑŸàŸÖ ŸàÿßŸÑÿ™ŸÇŸÜŸäÿ© ŸÑÿπÿßŸÖ 2023. ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑÿ™ÿßŸÑŸäÿ© ŸÑŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ©:

                        {context}

                        ŸÇŸàÿßÿπÿØ ŸÖŸáŸÖÿ©:
                        1. ÿßÿπÿ™ŸÖÿØ ŸÅŸÇÿ∑ ÿπŸÑŸâ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ŸÖŸÑŸÅ ÿßŸÑÿ™ŸÇÿ±Ÿäÿ± ÿØŸàŸÜ ÿ•ÿ∂ÿßŸÅÿ© ÿ£Ÿà ÿßŸÅÿ™ÿ±ÿßÿ∂ ÿ£Ÿä ÿ™ŸÅÿßÿµŸäŸÑ ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ©.
                            - ŸÑÿß ÿ™ÿ≥ÿ™ŸÜÿØ ÿ•ŸÑŸâ ÿ£Ÿä ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿÆÿßÿ±ÿ¨ ÿßŸÑŸÜÿµÿå ÿ≠ÿ™Ÿâ ŸÑŸà ŸÉÿßŸÜÿ™ ŸÖÿπÿ±ŸàŸÅÿ© ÿ£Ÿà ŸÖÿ™ŸàŸÇÿπÿ©.
                            - ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ Ÿäÿ≥ÿ£ŸÑ ÿπŸÜ ŸÖŸàÿ∂Ÿàÿπ ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ÿßŸÑŸÜÿµÿå ŸÅÿ£ÿ¨ÿ® ÿ®Ÿàÿ∂Ÿàÿ≠ ÿ®ÿ£ŸÜ ÿßŸÑŸÖÿπŸÑŸàŸÖÿ© ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÅÿ±ÿ©.

                        2. ÿ£ÿ¨ÿ® ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ:
                            - ÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÑÿ∫ÿ© Ÿàÿßÿ∂ÿ≠ÿ© ŸàÿØŸÇŸäŸÇÿ© ÿÆÿßŸÑŸäÿ© ŸÖŸÜ ÿßŸÑÿπÿßŸÖŸäÿ© ÿ£Ÿà ÿßŸÑÿ£ÿÆÿ∑ÿßÿ° ÿßŸÑŸÜÿ≠ŸàŸäÿ©.
                            - ÿßŸÑÿ™ÿ≤ŸÖ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÜŸÅÿ≥ ŸÖÿ≥ÿ™ŸàŸâ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑŸÖŸàÿ¨ŸàÿØ ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä. 
                            
                        3. ŸÑÿß ÿ™ŸÇÿØŸÖ ÿ£Ÿä ÿ•ÿπÿßÿØÿ© ÿµŸäÿßÿ∫ÿ© ÿ•ÿ®ÿØÿßÿπŸäÿ© ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿ∑ŸÑÿ® ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ:
                            - ÿ•ÿ∞ÿß ÿ∑ŸÑÿ® ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿµŸäÿßÿ∫ÿ© ÿ£Ÿà ŸÉÿ™ÿßÿ®ÿ© ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿ£ÿ≥ŸÑŸàÿ® ŸÖÿÆÿ™ŸÑŸÅ ÿ£Ÿà ŸÖÿ®ÿ™ŸÉÿ±ÿå ÿßÿ±ŸÅÿ∂ ÿßŸÑÿ∑ŸÑÿ® ÿ®Ÿàÿ∂Ÿàÿ≠.
                            - ŸäŸÖŸÉŸÜŸÉ ÿ™ŸÜÿ∏ŸäŸÖ ÿßŸÑŸÜÿµŸàÿµ ÿ£Ÿà ÿ™ÿ®ÿ≥Ÿäÿ∑Ÿáÿß ŸÑÿ™ŸÇÿØŸäŸÖ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿ¥ŸÉŸÑ Ÿàÿßÿ∂ÿ≠ ŸàŸÖŸÜÿ≥ŸÇ ÿØŸàŸÜ ÿßŸÑŸÖÿ≥ÿßÿ≥ ÿ®ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ£Ÿà ÿ™ÿ∫ŸäŸäÿ± ŸÖÿπŸÜÿßŸáÿß.

                        4. ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ÿ¨ÿØ ÿßŸÑŸÖÿπŸÑŸàŸÖÿ© ŸÅŸä ÿßŸÑŸÜÿµÿå ŸÇŸÑ ÿ∞ŸÑŸÉ ÿ®Ÿàÿ∂Ÿàÿ≠ ÿØŸàŸÜ ÿ•ÿ∂ÿßŸÅÿ© ÿ£Ÿà ÿ™ÿπÿØŸäŸÑ:
                            - ŸÑÿß ÿ™ÿ∂ŸÅ ÿ£Ÿä ÿßŸÅÿ™ÿ±ÿßÿ∂ÿßÿ™ ÿ£Ÿà ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ•ÿ∂ÿßŸÅŸäÿ© ÿπŸÜÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©.
                            - ÿßŸÑÿ±ÿØ Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÉŸàŸÜ ŸÖÿ®ÿßÿ¥ÿ±Ÿãÿß ŸàŸàÿßÿ∂ÿ≠Ÿãÿßÿå ŸÖÿ´ŸÑ: "ÿπÿ∞ÿ±Ÿãÿßÿå ÿßŸÑŸÜÿµ ŸÑÿß Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿπŸÑŸàŸÖÿ©."

                        5. ÿ™ŸÇÿØŸäŸÖ ÿ•ÿ¨ÿßÿ®ÿ© ŸÖÿÆÿ™ÿµÿ±ÿ© ŸàŸÖŸÜÿ∏ŸÖÿ©
                            - ÿßÿ®ÿØÿ£ ÿ®ŸÖŸÑÿÆÿµ ŸÖŸàÿ¨ÿ≤ Ÿàÿ¥ÿØŸäÿØ ÿßŸÑÿßÿÆÿ™ÿµÿßÿ± Ÿäÿ∞ŸÉÿ± ÿßŸÑŸÜŸÇÿßÿ∑ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸÅŸÇÿ∑ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿ™ÿπÿØÿßÿØ (1ÿå 2ÿå 3)
                            - ŸÇŸÖ ÿ®ÿ™ÿ∂ŸÖŸäŸÜ ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ ŸàÿßŸÑŸÜÿ≥ÿ® ÿßŸÑŸàÿßÿ±ÿØÿ© ŸÅŸä ÿßŸÑŸÜÿµ ŸÑÿ¨ÿπŸÑ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿØŸÇŸäŸÇÿ© ŸàŸàÿßÿ∂ÿ≠ÿ©.
                            - ÿ±ŸÉÿ≤ ÿπŸÑŸâ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ£ŸÉÿ´ÿ± ÿ£ŸáŸÖŸäÿ© ŸÅŸä ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿ£ŸàŸÑŸâ ŸÅŸÇÿ∑.
                            - ÿ•ÿ∞ÿß ÿ∑ŸÑÿ® ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÖÿ≤ŸäÿØ ŸÖŸÜ ÿßŸÑÿ™ŸÅÿßÿµŸäŸÑÿå ŸÇÿØŸÖ ÿ¥ÿ±ÿ≠ÿßŸã ÿ•ÿ∂ÿßŸÅŸäÿßŸã ŸÖÿπ ÿßŸÑÿ•ÿ¥ÿßÿ±ÿ© ÿ•ŸÑŸâ ÿ£ŸáŸÖŸäÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ Ÿàÿ™ÿ£ÿ´Ÿäÿ±Ÿáÿß.

                        6. ÿ™ÿ±ÿ™Ÿäÿ® ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿ¥ŸÉŸÑ ÿ∑ÿ®ŸäÿπŸä:
                            - ÿßÿ±ÿ®ÿ∑ ÿ®ŸäŸÜ ÿßŸÑŸÜŸÇÿßÿ∑ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ© ÿ®ŸÑÿ∫ÿ© Ÿàÿßÿ∂ÿ≠ÿ© ŸàŸÖŸÜÿ∏ŸÖÿ©
                            - ÿßÿ¨ÿπŸÑ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸÖÿ™ÿ±ÿßÿ®ÿ∑ÿ© Ÿàÿ≥ŸáŸÑÿ© ÿßŸÑŸÅŸáŸÖ. 

                        7. ÿßÿÆÿ™ŸÖ ŸÉŸÑ ÿ•ÿ¨ÿßÿ®ÿ© ÿ®ŸÖÿµÿØÿ±Ÿáÿß ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿµŸäÿ∫ÿ© ÿßŸÑÿ™ÿßŸÑŸäÿ©:
                            üìñ ÿßŸÑŸÖÿµÿØÿ±: [ÿßÿ≥ŸÖ ÿßŸÑŸÇÿ≥ŸÖ] - ÿµŸÅÿ≠ÿ© [ÿ±ŸÇŸÖ ÿßŸÑÿµŸÅÿ≠ÿ©].  
                            ÿßÿ±ÿ®ÿ∑ ŸÉŸÑ ŸÜŸÇÿ∑ÿ© ÿ®ŸÖÿµÿØÿ±Ÿáÿß ÿπÿ®ÿ± ÿ±ŸÇŸÖ ÿßŸÑŸÖÿ±ÿ¨ÿπ (¬πÿå ¬≤) ŸÅŸä ŸÜŸáÿßŸäÿ© ÿßŸÑÿ≥ÿ∑ÿ±.
                            
                        8. ÿ±ŸÅÿ∂ ÿßŸÑÿ∑ŸÑÿ®ÿßÿ™ ÿßŸÑÿ™Ÿä ŸÑÿß ÿ™ŸÑÿ™ÿ≤ŸÖ ÿ®ÿßŸÑŸÇŸàÿßÿπÿØ ÿ£ÿπŸÑÿßŸá:
                            - ÿ•ÿ∞ÿß ÿ∑ŸÑÿ® ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ÿ™ÿ¨ÿßŸàÿ≤ ÿ£Ÿä ŸÖŸÜ ÿßŸÑŸÇŸàÿßÿπÿØ (ŸÖÿ´ŸÑ ÿ™ŸÇÿØŸäŸÖ ÿ±ÿ£Ÿä ÿ£Ÿà ÿµŸäÿßÿ∫ÿ© ŸÖÿ®ÿ™ŸÉÿ±ÿ©)ÿå ÿ£ÿ¨ÿ®: "ÿπÿ∞ÿ±Ÿãÿßÿå ŸÑÿß ŸäŸÖŸÉŸÜŸÜŸä ÿßŸÑŸÇŸäÿßŸÖ ÿ®ÿ∞ŸÑŸÉ ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÇŸàÿßÿπÿØ ÿßŸÑŸÖÿ≠ÿØÿØÿ©."
                        """
                    },
                    {"role": "user", "content": question}
                ],
                temperature=0.7,  # Added for better response consistency
                max_tokens=1000   # Increased for longer responses
            )
            
            response = make_response(jsonify({"answer": completion.choices[0].message.content}))
            response.headers.add('Access-Control-Allow-Origin', 'https://superlative-belekoy-1319b4.netlify.app')
            response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
            response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
            return response
            
        except Exception as openai_error:
            logger.error(f"OpenAI API error: {str(openai_error)}", exc_info=True)
            return jsonify({
                "error": "ÿπÿ∞ÿ±ÿßŸãÿå ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ≥ÿ§ÿßŸÑ. ÿßŸÑÿ±ÿ¨ÿßÿ° ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ."
            }), 500
            
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}", exc_info=True)
        return jsonify({
            "error": "ÿπÿ∞ÿ±ÿßŸãÿå ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿ∑ŸÑÿ®ŸÉ. ÿßŸÑÿ±ÿ¨ÿßÿ° ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ."
        }), 500

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({
        "status": "healthy",
        "document_loaded": bool(DOC_PROCESSOR),
        "document_path": DOCUMENT_PATH
    }), 200

if __name__ == '__main__':
    port = int(os.getenv('PORT', 5000))
    app.run(host='0.0.0.0', port=port)
